{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import *\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.preprocessing import image\n",
    "from keras.models import *\n",
    "\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文件操作\n",
    "\n",
    "def move_files(source_path, target_path):\n",
    "    if not os.path.isdir(source_path):\n",
    "        print(\"source path is not exist\")\n",
    "        return\n",
    "    if not os.path.isdir(target_path):\n",
    "        os.makedirs(target_path)\n",
    "    files = os.listdir(source_path)\n",
    "    for file in files:\n",
    "        shutil.move(os.path.join(source_path,file), os.path.join(target_path,file))\n",
    "    shutil.rmtree(source_path)\n",
    "    print(\"finished moving\")\n",
    "\n",
    "               \n",
    "def extract_file(zip_file, data_path, extracted_path):\n",
    "    \n",
    "    if os.path.isdir(extracted_path):\n",
    "        print(\"Extracted files already exist.\")\n",
    "        return \n",
    "    if not os.path.isdir(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    try:\n",
    "        print('Extracting {}...'.format(zip_file))\n",
    "        with zipfile.ZipFile(zip_file) as zf:\n",
    "            zf.extractall(data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(data_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "    print(\"Finished extraction\")    \n",
    "    \n",
    "def move_files_into_sub_classes(extracted_path, new_path, classes):      \n",
    "    if not os.path.isdir(extracted_path):\n",
    "        print(\"The extracted folder is not exist.\")\n",
    "        return\n",
    "    \n",
    "    image_files = os.listdir(extracted_path)\n",
    "    if len(image_files)==0:\n",
    "        print(\"There is no file in the folder\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.isdir(new_path):\n",
    "        os.makedirs(new_path)\n",
    "        \n",
    "    sub_dirs = [os.path.join(new_path,each) for each in classes]\n",
    "    \n",
    "    for path in sub_dirs:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    print(\"Moving images to sub folders.\")\n",
    "    for image_file in image_files:\n",
    "        if classes[0] in image_file:\n",
    "            shutil.move(os.path.join(extracted_path,image_file), os.path.join(sub_dirs[0],image_file))\n",
    "        else:\n",
    "            shutil.move(os.path.join(extracted_path,image_file), os.path.join(sub_dirs[1],image_file))\n",
    "    shutil.rmtree(extracted_path)\n",
    "    print(\"Finish moving files to sub folders\")\n",
    "    \n",
    "def move_validation_files_from_trainset(train_path, validation_path, classes, split_index): \n",
    "    train_sub_paths = [os.path.join(train_path, class_item) for class_item in classes]\n",
    "    validation_sub_paths = [os.path.join(validation_path, class_item) for class_item in classes]        \n",
    "    \n",
    "    if os.path.isdir(validation_sub_paths[0]) and os.path.isdir(validation_sub_paths[1]):\n",
    "        print(\"Already moved\")\n",
    "        return \n",
    "    \n",
    "    for path in validation_sub_paths:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    for ii, image_file in enumerate(os.listdir(train_sub_paths[0]), 1):\n",
    "        if ii >split_index:\n",
    "            shutil.move(os.path.join(train_sub_paths[0],os.path.basename(image_file)),\n",
    "                        os.path.join(validation_sub_paths[0],os.path.basename(image_file)))\n",
    "    for ii, image_file in enumerate(os.listdir(train_sub_paths[1]), 1):\n",
    "        if ii >split_index:\n",
    "            shutil.move(os.path.join(train_sub_paths[1],os.path.basename(image_file)),\n",
    "                        os.path.join(validation_sub_paths[1],os.path.basename(image_file)))\n",
    "    print(\"Moving finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_shapes(file_path):\n",
    "    image_files = [os.path.join(file_path, file) for file in os.listdir(file_path)]\n",
    "    images_shapes= []\n",
    "    for ii, path in enumerate(image_files):\n",
    "        img = image.load_img(path)\n",
    "        shape = np.shape(img)\n",
    "        images_shapes.append(shape)\n",
    "    print(\"finished\")\n",
    "    return image_files, images_shapes\n",
    "\n",
    "\n",
    "#获取单个文件夹（只包含图片文件）中所有图片，用lambda_func预处理，用于异常值处理时读取图片\n",
    "def get_input_from_folder_with_image_files(file_path, img_size, lambda_func=None):\n",
    "    files = [os.path.join(file_path, file) for file in os.listdir(file_path)]\n",
    "    X = np.empty((len(files), img_size[0], img_size[1], 3), dtype=np.float32)\n",
    "    for ii, path in enumerate(files):\n",
    "        img = image.load_img(path, target_size=img_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = lambda_func(x)\n",
    "        X[ii]=x\n",
    "    print(\"finished\")\n",
    "    return X, files\n",
    "\n",
    "def get_basic_classifier_model(MODEL):\n",
    "    model = MODEL(weights=\"imagenet\")\n",
    "    return model\n",
    "\n",
    "def get_outliers_for_one_class(model, X_inputs, files, topN, nameList, lambda_decode):\n",
    "    preds = model.predict(X_inputs)\n",
    "    print(\"finished predicting\")\n",
    "    \n",
    "    decoded_preds = lambda_decode(preds, top=topN)\n",
    "    outliers = []\n",
    "    \n",
    "    for ii, predict in enumerate(decoded_preds, 0):\n",
    "        flag = False\n",
    "        for pre in enumerate(predict, 0):\n",
    "            if pre[1][0] in nameList:\n",
    "                flag=True\n",
    "                break\n",
    "        if flag == False:\n",
    "            outliers.append(files[ii])\n",
    "    print(len(outliers))\n",
    "    print(outliers[:10])\n",
    "    return outliers\n",
    "\n",
    "#可视化异常值等\n",
    "def visual_images(files, image_size):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import PIL\n",
    "    %matplotlib inline\n",
    "    \n",
    "    images =[image.load_img(path, target_size =image_size) for path in files]\n",
    "    images = [image.img_to_array(img) for img in images]\n",
    "    images = np.array(images).astype(np.uint8)\n",
    "    \n",
    "    batches = math.ceil(len(images)*1.0/8)\n",
    "    \n",
    "    print(batches)\n",
    "    images = images[0:batches*8]\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    for i in range(1, 8*batches+1 ):\n",
    "        if i-1 < len(files):\n",
    "            fig.add_subplot(batches, 8, i)\n",
    "            plt.xlabel(os.path.basename(files[i-1]))\n",
    "            plt.imshow(images[i-1])\n",
    "    plt.show()\n",
    "\n",
    "#将单个class里的outliers移到另一个文件夹\n",
    "def move_files_to_new_folder(files, target_path):\n",
    "    if len(files) == 0:\n",
    "        print(\"0 files, no moving\")\n",
    "        return \n",
    "    if not os.path.isdir(target_path):\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "    for file in files:\n",
    "        shutil.move(file, os.path.join(target_path,os.path.basename(file)))\n",
    "    print(\"finished moving\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取单个文件夹中（只包含子文件夹）中的所有图片，用lambda_func预处理，用作获取训练模型输入\n",
    "\n",
    "def get_train_input_from_folder_with_subclasses(train_path, img_size, lambda_func=None):\n",
    "    image_files = [[os.path.join(path, file) for file in os.listdir(path)] for path in train_path]\n",
    "    cat_num = len(image_files[0])\n",
    "    dog_num = len(image_files[1])\n",
    "    \n",
    "    image_files = np.concatenate(image_files, axis = 0)\n",
    "    \n",
    "    X_train = np.empty((cat_num+dog_num, img_size[0],img_size[1],3), dtype=np.float32)\n",
    "    y_train = np.concatenate((np.zeros((cat_num,1),dtype=np.float32),np.ones((dog_num,1),dtype=np.float32)), axis = 0)\n",
    "    \n",
    "    image_files, y_train = shuffle(image_files, y_train)\n",
    "    \n",
    "    for ii, path in enumerate(image_files):\n",
    "        img = image.load_img(path,target_size =img_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = lambda_func(x)\n",
    "        X_train[ii]=x\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(\"finished\")\n",
    "    return X_train, y_train, image_files\n",
    "\n",
    "\n",
    "\n",
    "def predict_and_update_to_csv(model, X_test, image_file_names, template_csv_path, target_csv_path):\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    y_pred = y_pred.clip(min=0.005, max=0.995)    \n",
    "        \n",
    "    df = pd.read_csv(template_csv_path)\n",
    "    \n",
    "    for i, fname in enumerate(image_file_names):\n",
    "        index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "        df.at[index-1,'label'] = y_pred[i]\n",
    "        \n",
    "    df.to_csv(target_csv_path, index=None)\n",
    "    print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gap(MODEL,image_size, train_path, test_path, save_path, lambda_func=None):\n",
    "    input_tensor = Input((image_size[0], image_size[1], 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    \n",
    "    base_model = MODEL(input_tensor=x, weights='imagenet', include_top=False)\n",
    "    model = Model(base_model.input, GlobalAveragePooling2D()(base_model.output))\n",
    "\n",
    "    gen = ImageDataGenerator()\n",
    "    train_generator = gen.flow_from_directory(train_path, image_size, shuffle=False, \n",
    "                                              batch_size=16)\n",
    "    test_generator = gen.flow_from_directory(test_path, image_size, shuffle=False, \n",
    "                                             batch_size=16, class_mode=None)\n",
    "\n",
    "    print(\"start to predict\")\n",
    "    train = model.predict_generator(train_generator)\n",
    "    test = model.predict_generator(test_generator)\n",
    "    print(\"finish to predict\")\n",
    "    with h5py.File(save_path) as h:\n",
    "        h.create_dataset(\"X_train\", data=train)\n",
    "        h.create_dataset(\"X_test\", data=test)\n",
    "        h.create_dataset(\"y_train\", data=train_generator.classes)\n",
    "    print(\"finished\")\n",
    "\n",
    "def load_and_merge_features(feature_files):\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    from sklearn.utils import shuffle\n",
    "    np.random.seed(2017)\n",
    "    \n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    \n",
    "    for filename in feature_files:\n",
    "        with h5py.File(filename, 'r') as h:\n",
    "            X_train.append(np.array(h['X_train']))\n",
    "            X_test.append(np.array(h['X_test']))\n",
    "            y_train = np.array(h['y_train'])\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis=1)\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    print('finished')\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def get_model_for_merge_features(X_input):    \n",
    "    input_tensor = Input(X_input.shape[1:])\n",
    "    x = Dropout(0.5)(input_tensor)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, x)\n",
    "\n",
    "    model.compile(optimizer='adadelta',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tuning_first_model(MODEL):\n",
    "    from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "    from keras.models import Model\n",
    "    \n",
    "    print(\"start\")\n",
    "    base_model = MODEL( weights='imagenet', include_top=False)\n",
    "    print(base_model.input.shape)\n",
    "    print(base_model.output.shape)\n",
    "\n",
    "    top_x = base_model.output\n",
    "    top_x = GlobalAveragePooling2D()(top_x)\n",
    "    top_x = Dropout(0.5)(top_x)\n",
    "    top_x = Dense(1, activation='sigmoid')(top_x)\n",
    "    model = Model(base_model.input, top_x)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_fine_tuning_second_model(model, layer_num):\n",
    "    for layer in model.layers[:layer_num]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[layer_num:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    from keras.optimizers import SGD\n",
    "\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def visualize_model(model, model_image):\n",
    "    from keras.utils.vis_utils import plot_model\n",
    "    from IPython.display import Image\n",
    "    \n",
    "    plot_model(model, to_file=model_image, show_shapes=True)\n",
    "    Image(model_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tflearn]",
   "language": "python",
   "name": "conda-env-tflearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
